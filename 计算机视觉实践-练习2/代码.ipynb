{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的包\n",
    "from download import download\n",
    "from mindspore.dataset import MnistDataset\n",
    "import mindspore.dataset as ds        \n",
    "import mindspore.dataset.transforms as C   \n",
    "import mindspore.dataset.vision as CV                \n",
    "from mindspore.dataset.vision import Inter      \n",
    "from mindspore import dtype as mstype\n",
    "import mindspore.nn as nn\n",
    "import mindspore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/datasets/MNIST_Data.zip (10.3 MB)\n",
      "\n",
      "file_sizes: 100%|██████████████████████████| 10.8M/10.8M [00:01<00:00, 9.37MB/s]\n",
      "Extracting zip file...\n",
      "Successfully downloaded / unzipped to ./\n"
     ]
    }
   ],
   "source": [
    "# 下载数据集\n",
    "url = \"https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/\" \\\n",
    "      \"notebook/datasets/MNIST_Data.zip\"\n",
    "path = download(url, \"./\", kind=\"zip\", replace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datapipe(data_path, batch_size=32):\n",
    "    # 加载数据集\n",
    "    mnist_ds = ds.MnistDataset(data_path)\n",
    "\n",
    "    # 定义所需要操作的预处理操作\n",
    "    resize_op = CV.Resize((32, 32), interpolation=Inter.LINEAR)     # 目标将图片大小调整为32*32，这样特征图能保证28*28，和原图一致\n",
    "    rescale_nml_op = CV.Rescale(1 / 0.3081 , -1 * 0.1307 / 0.3081)  # 数据集的标准化系数\n",
    "    rescale_op = CV.Rescale(1.0 / 255.0, 0.0)                       # 数据做标准化处理，所得到的数值分布满足正态分布\n",
    "    hwc2chw_op = CV.HWC2CHW()                                       # 转置操作\n",
    "    type_cast_op = C.TypeCast(mstype.int32)\n",
    "\n",
    "    # 使用map映射函数，将数据操作应用到数据集\n",
    "    mnist_ds = mnist_ds.map(operations=type_cast_op, input_columns=\"label\")\n",
    "    mnist_ds = mnist_ds.map(operations=[resize_op, rescale_op, rescale_nml_op, hwc2chw_op], input_columns=\"image\")\n",
    "\n",
    "    # 进行shuffle、batch操作\n",
    "    buffer_size = 10000\n",
    "    mnist_ds = mnist_ds.shuffle(buffer_size=buffer_size)\n",
    "    mnist_ds = mnist_ds.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    return mnist_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据处理\n",
    "train_dataset = datapipe('./MNIST_Data/train', 32)\n",
    "test_dataset = datapipe('./MNIST_Data/test', 32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LeNet模型\n",
    "class LeNet(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, pad_mode='valid')\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1, pad_mode='valid')\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Dense(16*5*5, 120)\n",
    "        self.fc2 = nn.Dense(120, 84)\n",
    "        self.fc3 = nn.Dense(84, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "    def construct(self, x):\n",
    "        x = self.relu(self.max_pool2d(self.conv1(x)))\n",
    "        x = self.relu(self.max_pool2d(self.conv2(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lenet\n",
    "model = LeNet()\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = nn.SGD(model.trainable_params(), 1e-2)\n",
    "\n",
    "# 向前传播\n",
    "def forward_fn(data, label):\n",
    "    logits = model(data)\n",
    "    loss = loss_fn(logits, label)\n",
    "    return loss, logits\n",
    "\n",
    "# 梯度计算函数\n",
    "grad_fn = mindspore.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=True)\n",
    "\n",
    "# 训练步骤函数\n",
    "def train_step(data, label):\n",
    "    (loss, _), grads = grad_fn(data, label)\n",
    "    optimizer(grads)\n",
    "    return loss\n",
    "\n",
    "def train(model, dataset):\n",
    "    size = dataset.get_dataset_size()\n",
    "    model.set_train()\n",
    "    for batch, (data, label) in enumerate(dataset.create_tuple_iterator()):\n",
    "        loss = train_step(data, label)\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.asnumpy(), batch\n",
    "            print(f\"loss: {loss:>7f}  [{current:>3d}/{size:>3d}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataset, loss_fn):\n",
    "    num_batches = dataset.get_dataset_size()\n",
    "    model.set_train(False)\n",
    "    total, test_loss, correct = 0, 0, 0\n",
    "    for data, label in dataset.create_tuple_iterator():\n",
    "        pred = model(data)\n",
    "        total += len(data)\n",
    "        test_loss += loss_fn(pred, label).asnumpy()\n",
    "        correct += (pred.argmax(1) == label).asnumpy().sum()\n",
    "    test_loss /= num_batches\n",
    "    correct /= total\n",
    "    print(f\"Test: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.212117  [  0/1875]\n",
      "loss: 0.001447  [100/1875]\n",
      "loss: 0.063531  [200/1875]\n",
      "loss: 0.026033  [300/1875]\n",
      "loss: 0.006008  [400/1875]\n",
      "loss: 0.043197  [500/1875]\n",
      "loss: 0.001799  [600/1875]\n",
      "loss: 0.005551  [700/1875]\n",
      "loss: 0.068206  [800/1875]\n",
      "loss: 0.050063  [900/1875]\n",
      "loss: 0.000482  [1000/1875]\n",
      "loss: 0.000850  [1100/1875]\n",
      "loss: 0.000387  [1200/1875]\n",
      "loss: 0.045702  [1300/1875]\n",
      "loss: 0.004178  [1400/1875]\n",
      "loss: 0.008348  [1500/1875]\n",
      "loss: 0.007381  [1600/1875]\n",
      "loss: 0.190744  [1700/1875]\n",
      "loss: 0.000445  [1800/1875]\n",
      "Test: \n",
      " Accuracy: 98.5%, Avg loss: 0.045345 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.006005  [  0/1875]\n",
      "loss: 0.000688  [100/1875]\n",
      "loss: 0.004597  [200/1875]\n",
      "loss: 0.002803  [300/1875]\n",
      "loss: 0.002727  [400/1875]\n",
      "loss: 0.011277  [500/1875]\n",
      "loss: 0.028346  [600/1875]\n",
      "loss: 0.009765  [700/1875]\n",
      "loss: 0.003821  [800/1875]\n",
      "loss: 0.004284  [900/1875]\n",
      "loss: 0.004983  [1000/1875]\n",
      "loss: 0.037588  [1100/1875]\n",
      "loss: 0.004987  [1200/1875]\n",
      "loss: 0.110214  [1300/1875]\n",
      "loss: 0.003540  [1400/1875]\n",
      "loss: 0.097777  [1500/1875]\n",
      "loss: 0.059304  [1600/1875]\n",
      "loss: 0.032477  [1700/1875]\n",
      "loss: 0.104789  [1800/1875]\n",
      "Test: \n",
      " Accuracy: 98.6%, Avg loss: 0.044864 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.000719  [  0/1875]\n",
      "loss: 0.001618  [100/1875]\n",
      "loss: 0.270643  [200/1875]\n",
      "loss: 0.033497  [300/1875]\n",
      "loss: 0.019740  [400/1875]\n",
      "loss: 0.007368  [500/1875]\n",
      "loss: 0.007030  [600/1875]\n",
      "loss: 0.006907  [700/1875]\n",
      "loss: 0.085566  [800/1875]\n",
      "loss: 0.050631  [900/1875]\n",
      "loss: 0.001565  [1000/1875]\n",
      "loss: 0.001163  [1100/1875]\n",
      "loss: 0.013475  [1200/1875]\n",
      "loss: 0.108859  [1300/1875]\n",
      "loss: 0.010697  [1400/1875]\n",
      "loss: 0.027841  [1500/1875]\n",
      "loss: 0.003321  [1600/1875]\n",
      "loss: 0.001071  [1700/1875]\n",
      "loss: 0.012643  [1800/1875]\n",
      "Test: \n",
      " Accuracy: 98.7%, Avg loss: 0.038709 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.031751  [  0/1875]\n",
      "loss: 0.105094  [100/1875]\n",
      "loss: 0.004001  [200/1875]\n",
      "loss: 0.012685  [300/1875]\n",
      "loss: 0.000396  [400/1875]\n",
      "loss: 0.195046  [500/1875]\n",
      "loss: 0.002272  [600/1875]\n",
      "loss: 0.008560  [700/1875]\n",
      "loss: 0.031195  [800/1875]\n",
      "loss: 0.047596  [900/1875]\n",
      "loss: 0.016804  [1000/1875]\n",
      "loss: 0.000766  [1100/1875]\n",
      "loss: 0.001628  [1200/1875]\n",
      "loss: 0.032594  [1300/1875]\n",
      "loss: 0.078076  [1400/1875]\n",
      "loss: 0.002947  [1500/1875]\n",
      "loss: 0.004963  [1600/1875]\n",
      "loss: 0.001154  [1700/1875]\n",
      "loss: 0.607629  [1800/1875]\n",
      "Test: \n",
      " Accuracy: 98.9%, Avg loss: 0.035053 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.019921  [  0/1875]\n",
      "loss: 0.002715  [100/1875]\n",
      "loss: 0.204819  [200/1875]\n",
      "loss: 0.003407  [300/1875]\n",
      "loss: 0.009991  [400/1875]\n",
      "loss: 0.014177  [500/1875]\n",
      "loss: 0.004761  [600/1875]\n",
      "loss: 0.002158  [700/1875]\n",
      "loss: 0.010476  [800/1875]\n",
      "loss: 0.002714  [900/1875]\n",
      "loss: 0.023092  [1000/1875]\n",
      "loss: 0.033183  [1100/1875]\n",
      "loss: 0.004402  [1200/1875]\n",
      "loss: 0.001632  [1300/1875]\n",
      "loss: 0.006789  [1400/1875]\n",
      "loss: 0.004973  [1500/1875]\n",
      "loss: 0.039301  [1600/1875]\n",
      "loss: 0.017721  [1700/1875]\n",
      "loss: 0.003129  [1800/1875]\n",
      "Test: \n",
      " Accuracy: 98.8%, Avg loss: 0.033561 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.011692  [  0/1875]\n",
      "loss: 0.069332  [100/1875]\n",
      "loss: 0.000382  [200/1875]\n",
      "loss: 0.112079  [300/1875]\n",
      "loss: 0.006234  [400/1875]\n",
      "loss: 0.004070  [500/1875]\n",
      "loss: 0.127376  [600/1875]\n",
      "loss: 0.002408  [700/1875]\n",
      "loss: 0.003254  [800/1875]\n",
      "loss: 0.127966  [900/1875]\n",
      "loss: 0.000353  [1000/1875]\n",
      "loss: 0.001126  [1100/1875]\n",
      "loss: 0.004170  [1200/1875]\n",
      "loss: 0.004063  [1300/1875]\n",
      "loss: 0.000930  [1400/1875]\n",
      "loss: 0.021444  [1500/1875]\n",
      "loss: 0.133503  [1600/1875]\n",
      "loss: 0.071846  [1700/1875]\n",
      "loss: 0.000439  [1800/1875]\n",
      "Test: \n",
      " Accuracy: 98.7%, Avg loss: 0.038847 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.002923  [  0/1875]\n",
      "loss: 0.002752  [100/1875]\n",
      "loss: 0.000324  [200/1875]\n",
      "loss: 0.001241  [300/1875]\n",
      "loss: 0.000864  [400/1875]\n",
      "loss: 0.000246  [500/1875]\n",
      "loss: 0.000473  [600/1875]\n",
      "loss: 0.000343  [700/1875]\n",
      "loss: 0.012408  [800/1875]\n",
      "loss: 0.038813  [900/1875]\n",
      "loss: 0.018554  [1000/1875]\n",
      "loss: 0.003069  [1100/1875]\n",
      "loss: 0.001133  [1200/1875]\n",
      "loss: 0.025572  [1300/1875]\n",
      "loss: 0.001080  [1400/1875]\n",
      "loss: 0.001344  [1500/1875]\n",
      "loss: 0.013216  [1600/1875]\n",
      "loss: 0.001917  [1700/1875]\n",
      "loss: 0.001087  [1800/1875]\n",
      "Test: \n",
      " Accuracy: 98.8%, Avg loss: 0.034922 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.020028  [  0/1875]\n",
      "loss: 0.000551  [100/1875]\n",
      "loss: 0.021472  [200/1875]\n",
      "loss: 0.000170  [300/1875]\n",
      "loss: 0.005998  [400/1875]\n",
      "loss: 0.016086  [500/1875]\n",
      "loss: 0.013164  [600/1875]\n",
      "loss: 0.017556  [700/1875]\n",
      "loss: 0.001952  [800/1875]\n",
      "loss: 0.002839  [900/1875]\n",
      "loss: 0.005147  [1000/1875]\n",
      "loss: 0.003121  [1100/1875]\n",
      "loss: 0.006135  [1200/1875]\n",
      "loss: 0.004245  [1300/1875]\n",
      "loss: 0.001266  [1400/1875]\n",
      "loss: 0.022749  [1500/1875]\n",
      "loss: 0.007830  [1600/1875]\n",
      "loss: 0.005478  [1700/1875]\n",
      "loss: 0.000646  [1800/1875]\n",
      "Test: \n",
      " Accuracy: 99.0%, Avg loss: 0.033741 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.004391  [  0/1875]\n",
      "loss: 0.009541  [100/1875]\n",
      "loss: 0.004034  [200/1875]\n",
      "loss: 0.000695  [300/1875]\n",
      "loss: 0.001035  [400/1875]\n",
      "loss: 0.001257  [500/1875]\n",
      "loss: 0.005225  [600/1875]\n",
      "loss: 0.001359  [700/1875]\n",
      "loss: 0.000962  [800/1875]\n",
      "loss: 0.004375  [900/1875]\n",
      "loss: 0.071188  [1000/1875]\n",
      "loss: 0.001546  [1100/1875]\n",
      "loss: 0.000702  [1200/1875]\n",
      "loss: 0.001317  [1300/1875]\n",
      "loss: 0.003817  [1400/1875]\n",
      "loss: 0.027239  [1500/1875]\n",
      "loss: 0.004117  [1600/1875]\n",
      "loss: 0.003029  [1700/1875]\n",
      "loss: 0.016798  [1800/1875]\n",
      "Test: \n",
      " Accuracy: 99.0%, Avg loss: 0.031995 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.006908  [  0/1875]\n",
      "loss: 0.001454  [100/1875]\n",
      "loss: 0.006896  [200/1875]\n",
      "loss: 0.020290  [300/1875]\n",
      "loss: 0.006164  [400/1875]\n",
      "loss: 0.000442  [500/1875]\n",
      "loss: 0.000391  [600/1875]\n",
      "loss: 0.000764  [700/1875]\n",
      "loss: 0.000102  [800/1875]\n",
      "loss: 0.002457  [900/1875]\n",
      "loss: 0.003411  [1000/1875]\n",
      "loss: 0.049359  [1100/1875]\n",
      "loss: 0.000010  [1200/1875]\n",
      "loss: 0.015109  [1300/1875]\n",
      "loss: 0.000296  [1400/1875]\n",
      "loss: 0.008566  [1500/1875]\n",
      "loss: 0.001639  [1600/1875]\n",
      "loss: 0.021303  [1700/1875]\n",
      "loss: 0.048712  [1800/1875]\n",
      "Test: \n",
      " Accuracy: 98.7%, Avg loss: 0.041301 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(model, train_dataset)\n",
    "    test(model, test_dataset, loss_fn)\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ms_37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
